{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install required library**"
      ],
      "metadata": {
        "id": "y86hEHhJcqP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers[torch] # Install transformers with torch support\n",
        "!pip install torch\n",
        "!pip install torchvision --no-cache-dir --force-reinstall --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install googletrans==3.1.0a0\n"
      ],
      "metadata": {
        "id": "aVngUYBscxLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading and downloading required dataset**"
      ],
      "metadata": {
        "id": "sR6Ce9XWc8Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "def load_squad_and_save_json(filename=\"squad_50.json\", num_samples=50):\n",
        "    # Load the SQuAD dataset.  We use the 'plain_text' version, which is easiest\n",
        "    # to work with for many applications.\n",
        "    squad_dataset = load_dataset(\"squad\", split=\"train\")\n",
        "\n",
        "    # Check if the dataset was loaded correctly\n",
        "    if not squad_dataset:\n",
        "        print(\"Error: Failed to load the SQuAD dataset.\")\n",
        "        return\n",
        "\n",
        "    # Get the total number of examples in the dataset\n",
        "    total_samples = len(squad_dataset)\n",
        "\n",
        "    # Check if the requested number of samples is valid\n",
        "    if num_samples > total_samples:\n",
        "        print(f\"Warning: Requested {num_samples} samples, but the dataset only contains {total_samples} samples.  Using the entire dataset.\")\n",
        "        num_samples = total_samples\n",
        "\n",
        "    # Create a list of indices, shuffle it, and take the first num_samples\n",
        "    indices = list(range(total_samples))\n",
        "    random.shuffle(indices)\n",
        "    selected_indices = indices[:num_samples]\n",
        "\n",
        "    # Create a new dataset with the selected samples\n",
        "    selected_dataset = squad_dataset.select(selected_indices)\n",
        "\n",
        "    # Convert the selected dataset to a list of dictionaries, which is easier to\n",
        "    # handle and convert to JSON.  We only need a few key fields.\n",
        "    data_list = []\n",
        "    for item in selected_dataset:\n",
        "        #  Handle potential errors or missing fields.  The SQuAD dataset\n",
        "        #  should be well-formed, but it's good practice to be robust.\n",
        "        if 'question' not in item or 'context' not in item or 'answers' not in item or 'id' not in item:\n",
        "            print(f\"Warning: Skipping item with missing fields: {item}\")\n",
        "            continue\n",
        "\n",
        "        question = item['question']\n",
        "        context = item['context']\n",
        "        answers = item['answers']\n",
        "        item_id = item['id']\n",
        "\n",
        "        # Further check the structure of the 'answers'\n",
        "        if not isinstance(answers, dict) or 'text' not in answers or 'answer_start' not in answers:\n",
        "             print(f\"Warning: Skipping item with invalid 'answers' format: {item['answers']}\")\n",
        "             continue\n",
        "        if not isinstance(answers['text'], list) or not isinstance(answers['answer_start'], list):\n",
        "             print(f\"Warning: Skipping item with invalid 'answers' sub-format: {item['answers']}\")\n",
        "             continue\n",
        "\n",
        "        # Ensure that the 'text' and 'answer_start' lists have the same length.\n",
        "        if len(answers['text']) != len(answers['answer_start']):\n",
        "            print(f\"Warning: Skipping item where 'answers' text and start lists have different lengths: {item['answers']}\")\n",
        "            continue\n",
        "\n",
        "        # Take only the first answer.  Some SQuAD examples have multiple\n",
        "        # possible answers, but for fine-tuning, we often simplify to a single answer.\n",
        "        if answers['text']:\n",
        "            first_answer_text = answers['text'][0]\n",
        "            first_answer_start = answers['answer_start'][0]\n",
        "        else:\n",
        "            print(f\"Warning: Skipping item with empty answer text list: {item['answers']}\")\n",
        "            continue\n",
        "\n",
        "        data_list.append({\n",
        "            'id': item_id,\n",
        "            'question': question,\n",
        "            'context': context,\n",
        "            'answer_text': first_answer_text,\n",
        "            'answer_start': first_answer_start,\n",
        "        })\n",
        "\n",
        "    # Convert the list of dictionaries to a JSON string\n",
        "    json_data = json.dumps(data_list, indent=4)\n",
        "\n",
        "    # Write the JSON string to a file\n",
        "    try:\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(json_data)\n",
        "        print(f\"Successfully saved {len(data_list)} samples to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to file {filename}: {e}\")\n",
        "        return\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    load_squad_and_save_json(num_samples=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUrjmgImfEaU",
        "outputId": "13c6b8d6-e730-4e91-da39-392fc8c095b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved 50 samples to squad_50.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# Fine Tuning and Evaluating Multilingual bert (English)**"
      ],
      "metadata": {
        "id": "pTP9rKU74DOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "from torch.optim import AdamW # Import AdamW from torch.optim\n",
        "from transformers import BertForQuestionAnswering, BertTokenizerFast, get_linear_schedule_with_warmup # Import BertTokenizerFast and get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "import re\n",
        "import string # Import the string module\n",
        "\n",
        "\n",
        "def compute_f1(predictions, references):\n",
        "\n",
        "    def normalize_string(s):\n",
        "        \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.strip().split())\n",
        "\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    f1_scores = []\n",
        "    for prediction, reference in zip(predictions, references):\n",
        "        # Handle empty predictions or references\n",
        "        if not prediction or not reference:\n",
        "            if not prediction and not reference:\n",
        "                f1_scores.append(1.0)  # Both empty, perfect match\n",
        "            else:\n",
        "                f1_scores.append(0.0)  # One empty, no match\n",
        "            continue\n",
        "\n",
        "        prediction_tokens = normalize_string(prediction).split()\n",
        "        reference_tokens = normalize_string(reference).split()\n",
        "\n",
        "        common = [w for w in prediction_tokens if w in reference_tokens]\n",
        "        num_same = len(common)\n",
        "\n",
        "        if num_same == 0:\n",
        "            f1_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        precision = num_same / len(prediction_tokens)\n",
        "        recall = num_same / len(reference_tokens)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        f1_scores.append(f1)\n",
        "    return sum(f1_scores) / len(f1_scores) if f1_scores else 0.0 # handle the case where the list is empty.\n",
        "\n",
        "\n",
        "def read_squad_json(file_path):\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_data(data, tokenizer, max_length=384, doc_stride=128):\n",
        "\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    example_ids = []\n",
        "\n",
        "    for example in data:\n",
        "        question_text = example['question']\n",
        "        context_text = example['context']\n",
        "        answer_text = example['answer_text']\n",
        "        example_id = example['id']\n",
        "\n",
        "        # Tokenize the question and context, handling long contexts with stride\n",
        "        tokenized_context = tokenizer.encode_plus(\n",
        "            context_text,\n",
        "            truncation=True,  # Don't truncate here, handle with sliding window\n",
        "            add_special_tokens=False,  # We'll add them later\n",
        "            return_offsets_mapping=True,  # Get character-to-token mappings\n",
        "            return_overflowing_tokens=True,  # Return long contexts as multiple features\n",
        "            max_length=max_length - len(tokenizer.encode(question_text, add_special_tokens=False)) - 3, # adjust max_length for context to account for question and special tokens\n",
        "            stride=doc_stride\n",
        "        )\n",
        "\n",
        "        # Tokenize the question\n",
        "        tokenized_question = tokenizer.encode_plus(\n",
        "            question_text,\n",
        "            add_special_tokens=False,\n",
        "            return_attention_mask=False,  # We'll create a combined mask later\n",
        "        )\n",
        "\n",
        "        # The 'overflowing_tokens' and 'offset_mapping' are the key to handling long contexts\n",
        "        for i in range(len(tokenized_context['input_ids'])):  # Loop through the features created from long context\n",
        "            # Get the input_ids, token_type_ids and attention mask for each context\n",
        "            context_input_ids = tokenized_context['input_ids'][i]\n",
        "            context_attention_mask = tokenized_context['attention_mask'][i] if 'attention_mask' in tokenized_context else [\n",
        "                                                                                                                      1] * len(\n",
        "                context_input_ids)\n",
        "            context_offsets = tokenized_context['offset_mapping'][i]\n",
        "\n",
        "            # Combine question and context, [CLS] question [SEP] context [SEP]\n",
        "            combined_input_ids = [tokenizer.cls_token_id] + tokenized_question['input_ids'] + [\n",
        "                tokenizer.sep_token_id] + context_input_ids + [tokenizer.sep_token_id]\n",
        "            combined_attention_mask = [1] + [1] * len(tokenized_question['input_ids']) + [1] + context_attention_mask + [\n",
        "                1]\n",
        "            # Pad to max_length\n",
        "            padding_length = max_length - len(combined_input_ids)\n",
        "            if padding_length > 0:\n",
        "                combined_input_ids = combined_input_ids + [tokenizer.pad_token_id] * padding_length\n",
        "                combined_attention_mask = combined_attention_mask + [0] * padding_length\n",
        "\n",
        "            # This is necessary if the question + context is still longer than max_length after doc_stride\n",
        "            # This truncation will cut off part of the context\n",
        "            if len(combined_input_ids) > max_length:\n",
        "                combined_input_ids = combined_input_ids[:max_length]\n",
        "                combined_attention_mask = combined_attention_mask[:max_length]\n",
        "            # Find the start and end positions of the answer in the context *within this span*\n",
        "            start_char = example['answer_start']\n",
        "            end_char = start_char + len(answer_text)\n",
        "\n",
        "            # Initialize to 0, in case we don't find a valid start/end\n",
        "            start_position = 0\n",
        "            end_position = 0\n",
        "\n",
        "            # Loop through the context offsets to find the *token* position of the answer\n",
        "            for idx, (start_offset, end_offset) in enumerate(context_offsets):\n",
        "                if start_offset <= start_char and end_offset >= start_char:\n",
        "                    start_position = len(tokenized_question['input_ids']) + 2 + idx  # +2 for [CLS], [SEP]\n",
        "                if start_offset <= end_char and end_offset >= end_char:\n",
        "                    end_position = len(tokenized_question['input_ids']) + 2 + idx\n",
        "\n",
        "            # Handle the case where the answer is out of the current context window\n",
        "            if start_position == 0 and end_position == 0:\n",
        "                start_position = 0\n",
        "                end_position = 0\n",
        "\n",
        "            input_ids.append(combined_input_ids)\n",
        "            attention_masks.append(combined_attention_mask)\n",
        "            start_positions.append(start_position)\n",
        "            end_positions.append(end_position)\n",
        "            example_ids.append(example_id)\n",
        "\n",
        "    return input_ids, attention_masks, start_positions, end_positions, example_ids\n",
        "\n",
        "\n",
        "def create_dataloader(input_ids, attention_masks, start_positions, end_positions, batch_size):\n",
        "\n",
        "    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "    all_attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "    all_start_positions = torch.tensor(start_positions, dtype=torch.long)\n",
        "    all_end_positions = torch.tensor(end_positions, dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_masks, all_start_positions, all_end_positions)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def fine_tune_and_evaluate(train_data, val_data, model_name=\"bert-base-multilingual-cased\",\n",
        "                            batch_size=10, epochs=3, learning_rate=3e-5,\n",
        "                            max_length=384, doc_stride=128):\n",
        "\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "    model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Preprocess the training and validation data\n",
        "    train_input_ids, train_attention_masks, train_start_positions, train_end_positions, train_example_ids = preprocess_data(\n",
        "        train_data, tokenizer, max_length, doc_stride)\n",
        "    val_input_ids, val_attention_masks, val_start_positions, val_end_positions, val_example_ids = preprocess_data(\n",
        "        val_data, tokenizer, max_length, doc_stride)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataloader = create_dataloader(train_input_ids, train_attention_masks, train_start_positions,\n",
        "                                         train_end_positions, batch_size)\n",
        "    val_dataloader = create_dataloader(val_input_ids, val_attention_masks, val_start_positions,\n",
        "                                       val_end_positions, batch_size)\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
        "                                                num_training_steps=total_steps)  # Changed variable name\n",
        "\n",
        "    # Fine-tuning loop\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"start_positions\": batch[2],\n",
        "                \"end_positions\": batch[3],\n",
        "            }\n",
        "            model.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "    # Evaluation on the validation set\n",
        "    model.eval()\n",
        "    print(\"Evaluating on the validation set...\")\n",
        "    all_predicted_answers = []\n",
        "    all_reference_answers = []\n",
        "    all_val_ids = []  # Collect IDs for matching\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "            }\n",
        "            outputs = model(**inputs)\n",
        "            start_logits = outputs.start_logits\n",
        "            end_logits = outputs.end_logits\n",
        "\n",
        "            # Get the predicted start and end positions\n",
        "            start_positions = torch.argmax(start_logits, dim=-1).cpu().tolist()\n",
        "            end_positions = torch.argmax(end_logits, dim=-1).cpu().tolist()\n",
        "\n",
        "            # Get the input IDs and convert them to text\n",
        "            input_ids_batch = batch[0].cpu().tolist()\n",
        "\n",
        "            # Decode the predicted answer\n",
        "            for i in range(len(input_ids_batch)):\n",
        "                input_ids = input_ids_batch[i]\n",
        "                start_pos = start_positions[i]\n",
        "                end_pos = end_positions[i]\n",
        "\n",
        "                # Ensure start_pos <= end_pos\n",
        "                if start_pos > end_pos:\n",
        "                    predicted_answer_text = \"\"  # Or some other default value\n",
        "                else:\n",
        "                    tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)\n",
        "                    predicted_answer_text = tokenizer.convert_tokens_to_string(\n",
        "                        tokens[start_pos:end_pos + 1])\n",
        "                all_predicted_answers.append(predicted_answer_text)\n",
        "\n",
        "        # Get the *reference* answers from the original validation data, matching by ID\n",
        "        for val_example in val_data:\n",
        "            all_val_ids.append(val_example['id'])\n",
        "\n",
        "        for val_id in val_example_ids:\n",
        "            index = all_val_ids.index(val_id)  # find the index\n",
        "            all_reference_answers.append(val_data[index]['answer_text'])\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = compute_f1(all_predicted_answers, all_reference_answers)\n",
        "    print(f\"F1 Score on the validation set: {f1:.4f}\")\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"fine_tuned_mbert_model\")\n",
        "    tokenizer.save_pretrained(\"fine_tuned_mbert_model\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your SQuAD-formatted JSON data\n",
        "    train_data = read_squad_json(\"squad_40.json\")\n",
        "    val_data = read_squad_json(\"squad_40.json\")  # using the same data for simplicity\n",
        "\n",
        "    # Fine-tune and evaluate the model\n",
        "    fine_tune_and_evaluate(train_data, val_data, model_name=\"bert-base-multilingual-cased\")\n"
      ],
      "metadata": {
        "id": "rtKODgZwVgon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Store the model_type, Dataset type and f1 score in dataframe**"
      ],
      "metadata": {
        "id": "ecY13vUXddR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initial DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Model': [\"Multilingual bert\"],\n",
        "    'Dataset': [\"English\"],\n",
        "    \"F1_Score\":[f1]\n",
        "})"
      ],
      "metadata": {
        "id": "KdobQxHNdcJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# Fine Tuning and Evaluating Monolingual bert (English)**"
      ],
      "metadata": {
        "id": "2e_CwVvb3-D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "from torch.optim import AdamW # Import AdamW from torch.optim\n",
        "from transformers import BertForQuestionAnswering, BertTokenizerFast, get_linear_schedule_with_warmup # Import BertTokenizerFast and get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "import re\n",
        "import string # Import the string module\n",
        "\n",
        "def compute_f1(predictions, references):\n",
        "\n",
        "    def normalize_string(s):\n",
        "        \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.strip().split())\n",
        "\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    f1_scores = []\n",
        "    for prediction, reference in zip(predictions, references):\n",
        "        # Handle empty predictions or references\n",
        "        if not prediction or not reference:\n",
        "            if not prediction and not reference:\n",
        "                f1_scores.append(1.0)  # Both empty, perfect match\n",
        "            else:\n",
        "                f1_scores.append(0.0)  # One empty, no match\n",
        "            continue\n",
        "\n",
        "        prediction_tokens = normalize_string(prediction).split()\n",
        "        reference_tokens = normalize_string(reference).split()\n",
        "\n",
        "        common = [w for w in prediction_tokens if w in reference_tokens]\n",
        "        num_same = len(common)\n",
        "\n",
        "        if num_same == 0:\n",
        "            f1_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        precision = num_same / len(prediction_tokens)\n",
        "        recall = num_same / len(reference_tokens)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        f1_scores.append(f1)\n",
        "    return sum(f1_scores) / len(f1_scores) if f1_scores else 0.0 # handle the case where the list is empty.\n",
        "\n",
        "\n",
        "def read_squad_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_data(data, tokenizer, max_length=384, doc_stride=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    example_ids = []\n",
        "\n",
        "    for example in data:\n",
        "        question_text = example['question']\n",
        "        context_text = example['context']\n",
        "        answer_text = example['answer_text']\n",
        "        example_id = example['id']\n",
        "\n",
        "        # Tokenize the question and context, handling long contexts with stride\n",
        "        tokenized_context = tokenizer.encode_plus(\n",
        "            context_text,\n",
        "            truncation=True,  # Don't truncate here, handle with sliding window\n",
        "            add_special_tokens=False,  # We'll add them later\n",
        "            return_offsets_mapping=True,  # Get character-to-token mappings\n",
        "            return_overflowing_tokens=True,  # Return long contexts as multiple features\n",
        "            max_length=max_length - len(tokenizer.encode(question_text, add_special_tokens=False)) - 3,  # adjust max_length for context to account for question and special tokens\n",
        "            stride=doc_stride\n",
        "        )\n",
        "\n",
        "        # Tokenize the question\n",
        "        tokenized_question = tokenizer.encode_plus(\n",
        "            question_text,\n",
        "            add_special_tokens=False,\n",
        "            return_attention_mask=False,  # We'll create a combined mask later\n",
        "        )\n",
        "\n",
        "        # The 'overflowing_tokens' and 'offset_mapping' are the key to handling long contexts\n",
        "        for i in range(len(tokenized_context['input_ids'])):  # Loop through the features created from long context\n",
        "            # Get the input_ids, token_type_ids and attention mask for each context\n",
        "            context_input_ids = tokenized_context['input_ids'][i]\n",
        "            context_attention_mask = tokenized_context['attention_mask'][i] if 'attention_mask' in tokenized_context else [\n",
        "                1] * len(\n",
        "                context_input_ids)\n",
        "            context_offsets = tokenized_context['offset_mapping'][i]\n",
        "\n",
        "            # Combine question and context, [CLS] question [SEP] context [SEP]\n",
        "            combined_input_ids = [tokenizer.cls_token_id] + tokenized_question['input_ids'] + [\n",
        "                tokenizer.sep_token_id] + context_input_ids + [tokenizer.sep_token_id]\n",
        "            combined_attention_mask = [1] + [1] * len(tokenized_question['input_ids']) + [1] + context_attention_mask + [\n",
        "                1]\n",
        "            # Pad to max_length\n",
        "            padding_length = max_length - len(combined_input_ids)\n",
        "            if padding_length > 0:\n",
        "                combined_input_ids = combined_input_ids + [tokenizer.pad_token_id] * padding_length\n",
        "                combined_attention_mask = combined_attention_mask + [0] * padding_length\n",
        "\n",
        "            # This is necessary if the question + context is still longer than max_length after doc_stride\n",
        "            # This truncation will cut off part of the context\n",
        "            if len(combined_input_ids) > max_length:\n",
        "                combined_input_ids = combined_input_ids[:max_length]\n",
        "                combined_attention_mask = combined_attention_mask[:max_length]\n",
        "            # Find the start and end positions of the answer in the context *within this span*\n",
        "            start_char = example['answer_start']\n",
        "            end_char = start_char + len(answer_text)\n",
        "\n",
        "            # Initialize to 0, in case we don't find a valid start/end\n",
        "            start_position = 0\n",
        "            end_position = 0\n",
        "\n",
        "            # Loop through the context offsets to find the *token* position of the answer\n",
        "            for idx, (start_offset, end_offset) in enumerate(context_offsets):\n",
        "                if start_offset <= start_char and end_offset >= start_char:\n",
        "                    start_position = len(tokenized_question['input_ids']) + 2 + idx  # +2 for [CLS], [SEP]\n",
        "                if start_offset <= end_char and end_offset >= end_char:\n",
        "                    end_position = len(tokenized_question['input_ids']) + 2 + idx\n",
        "\n",
        "            # Handle the case where the answer is out of the current context window\n",
        "            if start_position == 0 and end_position == 0:\n",
        "                start_position = 0\n",
        "                end_position = 0\n",
        "\n",
        "            input_ids.append(combined_input_ids)\n",
        "            attention_masks.append(combined_attention_mask)\n",
        "            start_positions.append(start_position)\n",
        "            end_positions.append(end_position)\n",
        "            example_ids.append(example_id)\n",
        "\n",
        "    return input_ids, attention_masks, start_positions, end_positions, example_ids\n",
        "\n",
        "\n",
        "def create_dataloader(input_ids, attention_masks, start_positions, end_positions, batch_size):\n",
        "\n",
        "    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "    all_attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "    all_start_positions = torch.tensor(start_positions, dtype=torch.long)\n",
        "    all_end_positions = torch.tensor(end_positions, dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_masks, all_start_positions, all_end_positions)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def fine_tune_and_evaluate(train_data, val_data, model_name=\"bert-base-uncased\",\n",
        "                        batch_size=10, epochs=3, learning_rate=3e-5,\n",
        "                        max_length=384, doc_stride=128):\n",
        "\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "    model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Preprocess the training and validation data\n",
        "    train_input_ids, train_attention_masks, train_start_positions, train_end_positions, train_example_ids = preprocess_data(\n",
        "        train_data, tokenizer, max_length, doc_stride)\n",
        "    val_input_ids, val_attention_masks, val_start_positions, val_end_positions, val_example_ids = preprocess_data(\n",
        "        val_data, tokenizer, max_length, doc_stride)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataloader = create_dataloader(train_input_ids, train_attention_masks, train_start_positions,\n",
        "                                        train_end_positions, batch_size)\n",
        "    val_dataloader = create_dataloader(val_input_ids, val_attention_masks, val_start_positions,\n",
        "                                      val_end_positions, batch_size)\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
        "                                                num_training_steps=total_steps)  # Changed variable name\n",
        "\n",
        "    # Fine-tuning loop\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"start_positions\": batch[2],\n",
        "                \"end_positions\": batch[3],\n",
        "            }\n",
        "            model.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "    # Evaluation on the validation set\n",
        "    model.eval()\n",
        "    print(\"Evaluating on the validation set...\")\n",
        "    all_predicted_answers = []\n",
        "    all_reference_answers = []\n",
        "    all_val_ids = []  # Collect IDs for matching\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "            }\n",
        "            outputs = model(**inputs)\n",
        "            start_logits = outputs.start_logits\n",
        "            end_logits = outputs.end_logits\n",
        "\n",
        "            # Get the predicted start and end positions\n",
        "            start_positions = torch.argmax(start_logits, dim=-1).cpu().tolist()\n",
        "            end_positions = torch.argmax(end_logits, dim=-1).cpu().tolist()\n",
        "\n",
        "            # Get the input IDs and convert them to text\n",
        "            input_ids_batch = batch[0].cpu().tolist()\n",
        "\n",
        "            # Decode the predicted answer\n",
        "            for i in range(len(input_ids_batch)):\n",
        "                input_ids = input_ids_batch[i]\n",
        "                start_pos = start_positions[i]\n",
        "                end_pos = end_positions[i]\n",
        "\n",
        "                # Ensure start_pos <= end_pos\n",
        "                if start_pos > end_pos:\n",
        "                    predicted_answer_text = \"\"  # Or some other default value\n",
        "                else:\n",
        "                    tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)\n",
        "                    predicted_answer_text = tokenizer.convert_tokens_to_string(\n",
        "                        tokens[start_pos:end_pos + 1])\n",
        "                all_predicted_answers.append(predicted_answer_text)\n",
        "\n",
        "        # Get the *reference* answers from the original validation data, matching by ID\n",
        "        for val_example in val_data:\n",
        "            all_val_ids.append(val_example['id'])\n",
        "\n",
        "        for val_id in val_example_ids:\n",
        "            index = all_val_ids.index(val_id)  # find the index\n",
        "            all_reference_answers.append(val_data[index]['answer_text'])\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = compute_f1(all_predicted_answers, all_reference_answers)\n",
        "    print(f\"F1 Score on the validation set: {f1*100:.4f}\")\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"fine_tuned_bert_model\")\n",
        "    tokenizer.save_pretrained(\"fine_tuned_bert_model\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your SQuAD-formatted JSON data\n",
        "    train_data = read_squad_json(\"squad_40.json\")\n",
        "    val_data = read_squad_json(\"squad_40.json\")  # using the same data for simplicity\n",
        "\n",
        "    # Fine-tune and evaluate the model\n",
        "    fine_tune_and_evaluate(train_data, val_data, model_name=\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "6C3yhjbDzdX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Store the model_type, Dataset type and f1 score in dataframe**"
      ],
      "metadata": {
        "id": "0ZYOX8sAecfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add dataframe with new row\n",
        "df.loc[len(df)] = ['Monolingual BERT', \"English\",f1]"
      ],
      "metadata": {
        "id": "meq1NGbCeZ6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Translate Data into Nepali**"
      ],
      "metadata": {
        "id": "rRy3DbeF6BqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "import json\n",
        "\n",
        "# Initialize the Google Translate API\n",
        "translator = Translator()\n",
        "\n",
        "# Function to translate and store the data in the required format\n",
        "def translate_squad_data(input_data):\n",
        "    translated_data = []\n",
        "\n",
        "    for item in input_data:\n",
        "        # Translate context, question, and answer to Nepali\n",
        "        nepali_context = translator.translate(item[\"context\"], src='en', dest='ne').text\n",
        "        nepali_question = translator.translate(item[\"question\"], src='en', dest='ne').text\n",
        "        # Access the answer text using 'answer_text' key\n",
        "        nepali_answer = translator.translate(item[\"answer_text\"], src='en', dest='ne').text\n",
        "\n",
        "        # Find the starting index of the translated answer in the translated context\n",
        "        new_answer_start = nepali_context.find(nepali_answer)\n",
        "        if new_answer_start == -1:\n",
        "            new_answer_start = 0  # Fallback if the exact match isn't found\n",
        "\n",
        "        # Append the translated data in the required format\n",
        "        translated_data.append({\n",
        "            \"id\": item['id'],\n",
        "            \"origan context\":item[\"context\"],\n",
        "            \"context\": nepali_context,\n",
        "            \"origan question\":item[\"question\"],\n",
        "            \"question\": nepali_question,\n",
        "            \"origan answer\":item[\"answer_text\"],\n",
        "            \"answer_text\": nepali_answer,\n",
        "            \"answer_start\": new_answer_start\n",
        "        })\n",
        "\n",
        "    return translated_data\n",
        "\n",
        "# Assuming you have the input SQuAD data in the following format\n",
        "# Load your SQuAD data (replace 'your_data_file.json' with the actual file)\n",
        "with open('squad_50.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Translate the SQuAD data\n",
        "translated_data = translate_squad_data(data)\n",
        "\n",
        "# Save the translated data to a new file\n",
        "with open('translated_squad_data_nepali.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(translated_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Optionally, download the translated data (for Google Colab)\n",
        "from google.colab import files\n",
        "files.download('translated_squad_data_nepali.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OslIFOGAYEz5",
        "outputId": "84921139-c656-445b-a2ba-55afdc1eec34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ca452279-fcbf-4b38-926c-a80fc3aac6d8\", \"translated_squad_data_nepali.json\", 137717)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine tune multilingual base BERT model with Nepali data set and evaluate**"
      ],
      "metadata": {
        "id": "9oAhmNmYNc-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "from torch.optim import AdamW # Import AdamW from torch.optim\n",
        "from transformers import BertForQuestionAnswering, BertTokenizerFast, get_linear_schedule_with_warmup # Import BertTokenizerFast and get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "import re\n",
        "import string # Import the string module\n",
        "\n",
        "\n",
        "def compute_f1(predictions, references):\n",
        "\n",
        "    def normalize_string(s):\n",
        "        \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.strip().split())\n",
        "\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    f1_scores = []\n",
        "    for prediction, reference in zip(predictions, references):\n",
        "        # Handle empty predictions or references\n",
        "        if not prediction or not reference:\n",
        "            if not prediction and not reference:\n",
        "                f1_scores.append(1.0)  # Both empty, perfect match\n",
        "            else:\n",
        "                f1_scores.append(0.0)  # One empty, no match\n",
        "            continue\n",
        "\n",
        "        prediction_tokens = normalize_string(prediction).split()\n",
        "        reference_tokens = normalize_string(reference).split()\n",
        "\n",
        "        common = [w for w in prediction_tokens if w in reference_tokens]\n",
        "        num_same = len(common)\n",
        "\n",
        "        if num_same == 0:\n",
        "            f1_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        precision = num_same / len(prediction_tokens)\n",
        "        recall = num_same / len(reference_tokens)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        f1_scores.append(f1)\n",
        "    return sum(f1_scores) / len(f1_scores) if f1_scores else 0.0 # handle the case where the list is empty.\n",
        "\n",
        "\n",
        "def read_squad_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_data(data, tokenizer, max_length=384, doc_stride=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    example_ids = []\n",
        "\n",
        "    for example in data:\n",
        "        question_text = example['question']\n",
        "        context_text = example['context']\n",
        "        answer_text = example['answer_text']\n",
        "        example_id = example['id']\n",
        "\n",
        "        # Tokenize the question and context, handling long contexts with stride\n",
        "        tokenized_context = tokenizer.encode_plus(\n",
        "            context_text,\n",
        "            truncation=True,  # Don't truncate here, handle with sliding window\n",
        "            add_special_tokens=False,  # We'll add them later\n",
        "            return_offsets_mapping=True,  # Get character-to-token mappings\n",
        "            return_overflowing_tokens=True,  # Return long contexts as multiple features\n",
        "            max_length=max_length - len(tokenizer.encode(question_text, add_special_tokens=False)) - 3, # adjust max_length for context to account for question and special tokens\n",
        "            stride=doc_stride\n",
        "        )\n",
        "\n",
        "        # Tokenize the question\n",
        "        tokenized_question = tokenizer.encode_plus(\n",
        "            question_text,\n",
        "            add_special_tokens=False,\n",
        "            return_attention_mask=False,  # We'll create a combined mask later\n",
        "        )\n",
        "\n",
        "        # The 'overflowing_tokens' and 'offset_mapping' are the key to handling long contexts\n",
        "        for i in range(len(tokenized_context['input_ids'])):  # Loop through the features created from long context\n",
        "            # Get the input_ids, token_type_ids and attention mask for each context\n",
        "            context_input_ids = tokenized_context['input_ids'][i]\n",
        "            context_attention_mask = tokenized_context['attention_mask'][i] if 'attention_mask' in tokenized_context else [\n",
        "                                                                                                                      1] * len(\n",
        "                context_input_ids)\n",
        "            context_offsets = tokenized_context['offset_mapping'][i]\n",
        "\n",
        "            # Combine question and context, [CLS] question [SEP] context [SEP]\n",
        "            combined_input_ids = [tokenizer.cls_token_id] + tokenized_question['input_ids'] + [\n",
        "                tokenizer.sep_token_id] + context_input_ids + [tokenizer.sep_token_id]\n",
        "            combined_attention_mask = [1] + [1] * len(tokenized_question['input_ids']) + [1] + context_attention_mask + [\n",
        "                1]\n",
        "            # Pad to max_length\n",
        "            padding_length = max_length - len(combined_input_ids)\n",
        "            if padding_length > 0:\n",
        "                combined_input_ids = combined_input_ids + [tokenizer.pad_token_id] * padding_length\n",
        "                combined_attention_mask = combined_attention_mask + [0] * padding_length\n",
        "\n",
        "            # This is necessary if the question + context is still longer than max_length after doc_stride\n",
        "            # This truncation will cut off part of the context\n",
        "            if len(combined_input_ids) > max_length:\n",
        "                combined_input_ids = combined_input_ids[:max_length]\n",
        "                combined_attention_mask = combined_attention_mask[:max_length]\n",
        "            # Find the start and end positions of the answer in the context *within this span*\n",
        "            start_char = example['answer_start']\n",
        "            end_char = start_char + len(answer_text)\n",
        "\n",
        "            # Initialize to 0, in case we don't find a valid start/end\n",
        "            start_position = 0\n",
        "            end_position = 0\n",
        "\n",
        "            # Loop through the context offsets to find the *token* position of the answer\n",
        "            for idx, (start_offset, end_offset) in enumerate(context_offsets):\n",
        "                if start_offset <= start_char and end_offset >= start_char:\n",
        "                    start_position = len(tokenized_question['input_ids']) + 2 + idx  # +2 for [CLS], [SEP]\n",
        "                if start_offset <= end_char and end_offset >= end_char:\n",
        "                    end_position = len(tokenized_question['input_ids']) + 2 + idx\n",
        "\n",
        "            # Handle the case where the answer is out of the current context window\n",
        "            if start_position == 0 and end_position == 0:\n",
        "                start_position = 0\n",
        "                end_position = 0\n",
        "\n",
        "            input_ids.append(combined_input_ids)\n",
        "            attention_masks.append(combined_attention_mask)\n",
        "            start_positions.append(start_position)\n",
        "            end_positions.append(end_position)\n",
        "            example_ids.append(example_id)\n",
        "\n",
        "    return input_ids, attention_masks, start_positions, end_positions, example_ids\n",
        "\n",
        "\n",
        "def create_dataloader(input_ids, attention_masks, start_positions, end_positions, batch_size):\n",
        "    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "    all_attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "    all_start_positions = torch.tensor(start_positions, dtype=torch.long)\n",
        "    all_end_positions = torch.tensor(end_positions, dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_masks, all_start_positions, all_end_positions)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def fine_tune_and_evaluate(train_data, val_data, model_name=\"bert-base-multilingual-cased\",\n",
        "                            batch_size=5, epochs=5, learning_rate=3e-5,\n",
        "                            max_length=384, doc_stride=128):\n",
        "\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "    model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Preprocess the training and validation data\n",
        "    train_input_ids, train_attention_masks, train_start_positions, train_end_positions, train_example_ids = preprocess_data(\n",
        "        train_data, tokenizer, max_length, doc_stride)\n",
        "    val_input_ids, val_attention_masks, val_start_positions, val_end_positions, val_example_ids = preprocess_data(\n",
        "        val_data, tokenizer, max_length, doc_stride)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataloader = create_dataloader(train_input_ids, train_attention_masks, train_start_positions,\n",
        "                                         train_end_positions, batch_size)\n",
        "    val_dataloader = create_dataloader(val_input_ids, val_attention_masks, val_start_positions,\n",
        "                                       val_end_positions, batch_size)\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
        "                                                num_training_steps=total_steps)  # Changed variable name\n",
        "\n",
        "    # Fine-tuning loop\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"start_positions\": batch[2],\n",
        "                \"end_positions\": batch[3],\n",
        "            }\n",
        "            model.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "    # Evaluation on the validation set\n",
        "    model.eval()\n",
        "    print(\"Evaluating on the validation set...\")\n",
        "    all_predicted_answers = []\n",
        "    all_reference_answers = []\n",
        "    all_val_ids = []  # Collect IDs for matching\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "            }\n",
        "            outputs = model(**inputs)\n",
        "            start_logits = outputs.start_logits\n",
        "            end_logits = outputs.end_logits\n",
        "\n",
        "            # Get the predicted start and end positions\n",
        "            start_positions = torch.argmax(start_logits, dim=-1).cpu().tolist()\n",
        "            end_positions = torch.argmax(end_logits, dim=-1).cpu().tolist()\n",
        "\n",
        "            # Get the input IDs and convert them to text\n",
        "            input_ids_batch = batch[0].cpu().tolist()\n",
        "\n",
        "            # Decode the predicted answer\n",
        "            for i in range(len(input_ids_batch)):\n",
        "                input_ids = input_ids_batch[i]\n",
        "                start_pos = start_positions[i]\n",
        "                end_pos = end_positions[i]\n",
        "\n",
        "                # Ensure start_pos <= end_pos\n",
        "                if start_pos > end_pos:\n",
        "                    predicted_answer_text = \"\"  # Or some other default value\n",
        "                else:\n",
        "                    tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)\n",
        "                    predicted_answer_text = tokenizer.convert_tokens_to_string(\n",
        "                        tokens[start_pos:end_pos + 1])\n",
        "                all_predicted_answers.append(predicted_answer_text)\n",
        "\n",
        "        # Get the *reference* answers from the original validation data, matching by ID\n",
        "        for val_example in val_data:\n",
        "            all_val_ids.append(val_example['id'])\n",
        "\n",
        "        for val_id in val_example_ids:\n",
        "            index = all_val_ids.index(val_id)  # find the index\n",
        "            all_reference_answers.append(val_data[index]['answer_text'])\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = compute_f1(all_predicted_answers, all_reference_answers)\n",
        "    print(f\"F1 Score on the validation set: {f1*10:.4f}\")\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"fine_tuned_mbert_model\")\n",
        "    tokenizer.save_pretrained(\"fine_tuned_mbert_model\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your SQuAD-formatted JSON data\n",
        "    train_data = read_squad_json(\"translated_squad_data_nepali.json\")\n",
        "    val_data = read_squad_json(\"translated_squad_data_nepali.json\") # using the same data for simplicity\n",
        "\n",
        "    # Fine-tune and evaluate the model\n",
        "    fine_tune_and_evaluate(train_data, val_data, model_name=\"bert-base-multilingual-cased\")\n"
      ],
      "metadata": {
        "id": "Ysvj4KQNck5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Store the model_type, Dataset type and f1 score in dataframe**"
      ],
      "metadata": {
        "id": "JA55BIEnhPvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add dataframe with new row\n",
        "df.loc[len(df)] = ['Multilingual bert', \"Nepali\",f1]"
      ],
      "metadata": {
        "id": "LIjIhoIEclCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine tune Monolingual BERT (NepBert) with Nepali data set and evaluate**"
      ],
      "metadata": {
        "id": "RiE0NpsiNsJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "from torch.optim import AdamW # Import AdamW from torch.optim\n",
        "from transformers import BertForQuestionAnswering, BertTokenizerFast, get_linear_schedule_with_warmup # Import BertTokenizerFast and get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "import re\n",
        "import string # Import the string module\n",
        "\n",
        "\n",
        "def compute_f1(predictions, references):\n",
        "\n",
        "    def normalize_string(s):\n",
        "        \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "        def remove_articles(text):\n",
        "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "        def white_space_fix(text):\n",
        "            return ' '.join(text.strip().split())\n",
        "\n",
        "        def remove_punc(text):\n",
        "            exclude = set(string.punctuation)\n",
        "            return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "        def lower(text):\n",
        "            return text.lower()\n",
        "\n",
        "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "    f1_scores = []\n",
        "    for prediction, reference in zip(predictions, references):\n",
        "        # Handle empty predictions or references\n",
        "        if not prediction or not reference:\n",
        "            if not prediction and not reference:\n",
        "                f1_scores.append(1.0)  # Both empty, perfect match\n",
        "            else:\n",
        "                f1_scores.append(0.0)  # One empty, no match\n",
        "            continue\n",
        "\n",
        "        prediction_tokens = normalize_string(prediction).split()\n",
        "        reference_tokens = normalize_string(reference).split()\n",
        "\n",
        "        common = [w for w in prediction_tokens if w in reference_tokens]\n",
        "        num_same = len(common)\n",
        "\n",
        "        if num_same == 0:\n",
        "            f1_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        precision = num_same / len(prediction_tokens)\n",
        "        recall = num_same / len(reference_tokens)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "        f1_scores.append(f1)\n",
        "    return sum(f1_scores) / len(f1_scores) if f1_scores else 0.0 # handle the case where the list is empty.\n",
        "\n",
        "\n",
        "def read_squad_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_data(data, tokenizer, max_length=384, doc_stride=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    example_ids = []\n",
        "\n",
        "    for example in data:\n",
        "        question_text = example['question']\n",
        "        context_text = example['context']\n",
        "        answer_text = example['answer_text']\n",
        "        example_id = example['id']\n",
        "\n",
        "        # Tokenize the question and context, handling long contexts with stride\n",
        "        tokenized_context = tokenizer.encode_plus(\n",
        "            context_text,\n",
        "            truncation=True,  # Don't truncate here, handle with sliding window\n",
        "            add_special_tokens=False,  # We'll add them later\n",
        "            return_offsets_mapping=True,  # Get character-to-token mappings\n",
        "            return_overflowing_tokens=True,  # Return long contexts as multiple features\n",
        "            max_length=max_length - len(tokenizer.encode(question_text, add_special_tokens=False)) - 3, # adjust max_length for context to account for question and special tokens\n",
        "            stride=doc_stride\n",
        "        )\n",
        "\n",
        "        # Tokenize the question\n",
        "        tokenized_question = tokenizer.encode_plus(\n",
        "            question_text,\n",
        "            add_special_tokens=False,\n",
        "            return_attention_mask=False,  # We'll create a combined mask later\n",
        "        )\n",
        "\n",
        "        # The 'overflowing_tokens' and 'offset_mapping' are the key to handling long contexts\n",
        "        for i in range(len(tokenized_context['input_ids'])):  # Loop through the features created from long context\n",
        "            # Get the input_ids, token_type_ids and attention mask for each context\n",
        "            context_input_ids = tokenized_context['input_ids'][i]\n",
        "            context_attention_mask = tokenized_context['attention_mask'][i] if 'attention_mask' in tokenized_context else [\n",
        "                                                                                                                      1] * len(\n",
        "                context_input_ids)\n",
        "            context_offsets = tokenized_context['offset_mapping'][i]\n",
        "\n",
        "            # Combine question and context, [CLS] question [SEP] context [SEP]\n",
        "            combined_input_ids = [tokenizer.cls_token_id] + tokenized_question['input_ids'] + [\n",
        "                tokenizer.sep_token_id] + context_input_ids + [tokenizer.sep_token_id]\n",
        "            combined_attention_mask = [1] + [1] * len(tokenized_question['input_ids']) + [1] + context_attention_mask + [\n",
        "                1]\n",
        "            # Pad to max_length\n",
        "            padding_length = max_length - len(combined_input_ids)\n",
        "            if padding_length > 0:\n",
        "                combined_input_ids = combined_input_ids + [tokenizer.pad_token_id] * padding_length\n",
        "                combined_attention_mask = combined_attention_mask + [0] * padding_length\n",
        "\n",
        "            # This is necessary if the question + context is still longer than max_length after doc_stride\n",
        "            # This truncation will cut off part of the context\n",
        "            if len(combined_input_ids) > max_length:\n",
        "                combined_input_ids = combined_input_ids[:max_length]\n",
        "                combined_attention_mask = combined_attention_mask[:max_length]\n",
        "            # Find the start and end positions of the answer in the context *within this span*\n",
        "            start_char = example['answer_start']\n",
        "            end_char = start_char + len(answer_text)\n",
        "\n",
        "            # Initialize to 0, in case we don't find a valid start/end\n",
        "            start_position = 0\n",
        "            end_position = 0\n",
        "\n",
        "            # Loop through the context offsets to find the *token* position of the answer\n",
        "            for idx, (start_offset, end_offset) in enumerate(context_offsets):\n",
        "                if start_offset <= start_char and end_offset >= start_char:\n",
        "                    start_position = len(tokenized_question['input_ids']) + 2 + idx  # +2 for [CLS], [SEP]\n",
        "                if start_offset <= end_char and end_offset >= end_char:\n",
        "                    end_position = len(tokenized_question['input_ids']) + 2 + idx\n",
        "\n",
        "            # Handle the case where the answer is out of the current context window\n",
        "            if start_position == 0 and end_position == 0:\n",
        "                start_position = 0\n",
        "                end_position = 0\n",
        "\n",
        "            input_ids.append(combined_input_ids)\n",
        "            attention_masks.append(combined_attention_mask)\n",
        "            start_positions.append(start_position)\n",
        "            end_positions.append(end_position)\n",
        "            example_ids.append(example_id)\n",
        "\n",
        "    return input_ids, attention_masks, start_positions, end_positions, example_ids\n",
        "\n",
        "\n",
        "def create_dataloader(input_ids, attention_masks, start_positions, end_positions, batch_size):\n",
        "    all_input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "    all_attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "    all_start_positions = torch.tensor(start_positions, dtype=torch.long)\n",
        "    all_end_positions = torch.tensor(end_positions, dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_masks, all_start_positions, all_end_positions)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def fine_tune_and_evaluate(train_data, val_data, model_name=\"Rajan/NepaliBERT\",\n",
        "                            batch_size=5, epochs=5, learning_rate=3e-5,\n",
        "                            max_length=384, doc_stride=128):\n",
        "\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "    model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Preprocess the training and validation data\n",
        "    train_input_ids, train_attention_masks, train_start_positions, train_end_positions, train_example_ids = preprocess_data(\n",
        "        train_data, tokenizer, max_length, doc_stride)\n",
        "    val_input_ids, val_attention_masks, val_start_positions, val_end_positions, val_example_ids = preprocess_data(\n",
        "        val_data, tokenizer, max_length, doc_stride)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataloader = create_dataloader(train_input_ids, train_attention_masks, train_start_positions,\n",
        "                                         train_end_positions, batch_size)\n",
        "    val_dataloader = create_dataloader(val_input_ids, val_attention_masks, val_start_positions,\n",
        "                                       val_end_positions, batch_size)\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
        "                                                num_training_steps=total_steps)  # Changed variable name\n",
        "\n",
        "    # Fine-tuning loop\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"start_positions\": batch[2],\n",
        "                \"end_positions\": batch[3],\n",
        "            }\n",
        "            model.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "    # Evaluation on the validation set\n",
        "    model.eval()\n",
        "    print(\"Evaluating on the validation set...\")\n",
        "    all_predicted_answers = []\n",
        "    all_reference_answers = []\n",
        "    all_val_ids = []  # Collect IDs for matching\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "            }\n",
        "            outputs = model(**inputs)\n",
        "            start_logits = outputs.start_logits\n",
        "            end_logits = outputs.end_logits\n",
        "\n",
        "            # Get the predicted start and end positions\n",
        "            start_positions = torch.argmax(start_logits, dim=-1).cpu().tolist()\n",
        "            end_positions = torch.argmax(end_logits, dim=-1).cpu().tolist()\n",
        "\n",
        "            # Get the input IDs and convert them to text\n",
        "            input_ids_batch = batch[0].cpu().tolist()\n",
        "\n",
        "            # Decode the predicted answer\n",
        "            for i in range(len(input_ids_batch)):\n",
        "                input_ids = input_ids_batch[i]\n",
        "                start_pos = start_positions[i]\n",
        "                end_pos = end_positions[i]\n",
        "\n",
        "                # Ensure start_pos <= end_pos\n",
        "                if start_pos > end_pos:\n",
        "                    predicted_answer_text = \"\"  # Or some other default value\n",
        "                else:\n",
        "                    tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)\n",
        "                    predicted_answer_text = tokenizer.convert_tokens_to_string(\n",
        "                        tokens[start_pos:end_pos + 1])\n",
        "                all_predicted_answers.append(predicted_answer_text)\n",
        "\n",
        "        # Get the *reference* answers from the original validation data, matching by ID\n",
        "        for val_example in val_data:\n",
        "            all_val_ids.append(val_example['id'])\n",
        "\n",
        "        for val_id in val_example_ids:\n",
        "            index = all_val_ids.index(val_id)  # find the index\n",
        "            all_reference_answers.append(val_data[index]['answer_text'])\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = compute_f1(all_predicted_answers, all_reference_answers)\n",
        "    print(f\"F1 Score on the validation set: {f1*10:.4f}\")\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"fine_tuned_mbert_model\")\n",
        "    tokenizer.save_pretrained(\"fine_tuned_mbert_model\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your SQuAD-formatted JSON data\n",
        "    train_data = read_squad_json(\"translated_squad_data_nepali.json\")\n",
        "    val_data = read_squad_json(\"translated_squad_data_nepali.json\") # using the same data for simplicity\n",
        "\n",
        "    # Fine-tune and evaluate the model\n",
        "    fine_tune_and_evaluate(train_data, val_data, model_name=\"Rajan/NepaliBERT\")\n"
      ],
      "metadata": {
        "id": "dt6pIjGkCIJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Store the model_type, Dataset type and f1 score in dataframe**"
      ],
      "metadata": {
        "id": "S50h6HsdhXmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add dataframe with new row\n",
        "df.loc[len(df)] = ['Monolingual BERT', \"Nepali\",f1]"
      ],
      "metadata": {
        "id": "O0yEXT5DCIRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Jn2vrJEOclIS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "a6731f1d-f881-40fe-9be3-45636fec0f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Model  Dataset  F1_Score\n",
              "0  Multilingual bert  English    0.0460\n",
              "1   Monolingual BERT  English    0.0880\n",
              "2  Multilingual bert   Nepali    0.0556\n",
              "3   Monolingual BERT   Nepali    0.0055"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ca528a0b-966e-4deb-91ba-a891338c81eb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Dataset</th>\n",
              "      <th>F1_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Multilingual bert</td>\n",
              "      <td>English</td>\n",
              "      <td>0.0460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Monolingual BERT</td>\n",
              "      <td>English</td>\n",
              "      <td>0.0880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Multilingual bert</td>\n",
              "      <td>Nepali</td>\n",
              "      <td>0.0556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Monolingual BERT</td>\n",
              "      <td>Nepali</td>\n",
              "      <td>0.0055</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca528a0b-966e-4deb-91ba-a891338c81eb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ca528a0b-966e-4deb-91ba-a891338c81eb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ca528a0b-966e-4deb-91ba-a891338c81eb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2cda933b-8e81-4c0b-8b34-06b93ad0b9ce\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2cda933b-8e81-4c0b-8b34-06b93ad0b9ce')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2cda933b-8e81-4c0b-8b34-06b93ad0b9ce button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Monolingual BERT\",\n          \"Multilingual bert\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dataset\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Nepali\",\n          \"English\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.033988270035410745,\n        \"min\": 0.0055,\n        \"max\": 0.088,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.088,\n          0.0055\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x='Dataset', y='F1_Score', hue='Model', data=df)\n",
        "plt.title('F1 Score by Model and Dataset')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.xlabel('Dataset')\n",
        "plt.legend(title='Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "faOU7la3jsV9",
        "outputId": "3d3151db-2536-4fec-b381-c9bf2fca182d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5KgSbTpXkQW4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}